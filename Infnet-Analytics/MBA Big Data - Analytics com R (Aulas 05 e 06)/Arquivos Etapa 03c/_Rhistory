install.packages(c("MASS", "survival"))
install.packages(c("MASS", "survival"))
install.packages("sparklyr")
library("DBI", lib.loc="~/R/win-library/3.5")
library("dbplyr", lib.loc="~/R/win-library/3.5")
library("sparklyr", lib.loc="~/R/win-library/3.5")
library("DBI", lib.loc="~/R/win-library/3.5")
detach("package:dbplyr", unload=TRUE)
library("dbplyr", lib.loc="~/R/win-library/3.5")
library("sparklyr", lib.loc="~/R/win-library/3.5")
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jdk1.8.0_161\\bin")
install.packages("dplyr")
install.packages("sparklyr")
install.packages("DBI")
library(DBI)
library(sparklyr)
install.packages("stringi")
library(DBI)
library(sparklyr)
library(dplyr)
package:stats
spark_install(version = "2.1")
spark_available_versions()
spark_install(version = "2.2.1")
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
if(Sys.getenv("JAVA_HOME")!="") Sys.setenv(JAVA_HOME="")
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jre1.8.0_171\\bin")
#Sys.setenv(JAVA_HOME="C:\\Program Files (x86)\\Java\\jre1.8.0_171\\bin")
# https://d.cosx.org/d/419485-sparklyr-spark-2-1-0/6
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
if(Sys.getenv("JAVA_HOME")!="") Sys.setenv(JAVA_HOME="")
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jdk1.8.0_171\\bin")
install.packages("dplyr")
install.packages("sparklyr")
install.packages("DBI")
install.packages("stringi")
library(DBI)
library(sparklyr)
library(dplyr)
spark_install(version = "2.2.1")
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
spark_available_versions()
spark_install(version = "2.1.0")
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
spark_install(version = "2.3.0")
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
shiny::runGadget(sparklyr::connection_spark_shinyapp(), viewer = .rs.embeddedViewer)
library(DBI)
library(sparklyr)
library(dplyr)
#spark_available_versions()
spark_install(version = "2.3.0")
#Cria uma conexÃ£o com spark tendo como master a mÃ¡quina local
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
library(DBI)
library(sparklyr)
library(dplyr)
#spark_available_versions()
spark_install(version = "2.3.0")
#Cria uma conexÃ£o com spark tendo como master a mÃ¡quina local
sc <- spark_connect(master = "local", app_name = "AulaInfnet")
shiny::runGadget(sparklyr::connection_spark_shinyapp(), viewer = .rs.embeddedViewer)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local")
install.packages("stringi")
setwd("C:/Users/barbosa/Google Drive/_Pos-graduacao/Bloco A/Analytics - Cassius Figueredo/MBA Big Data - Analytics com R (Aulas 05 e 06)/Arquivos Etapa 03a")
install.packages("dplyr", dependencies=TRUE)
#install.packages("dplyr", dependencies=TRUE)
install.packages("reshape2", dependencies=TRUE)
#install.packages("dplyr", dependencies=TRUE)
#install.packages("reshape2", dependencies=TRUE)
nstall.packages("tidyr", dependencies=TRUE)
#install.packages("dplyr", dependencies=TRUE)
#install.packages("reshape2", dependencies=TRUE)
install.packages("tidyr", dependencies=TRUE)
# Cria um dataframe aleatório.
set.seed(54321)
x <- matrix(rnorm(9), ncol = 3)
View(x)
View(x)
View(x)
# Calcula a média das linhas, linha a linha.
apply(x, 1, mean)
# Calcula a média das linhas, linha a linha.
xa1 <- apply(x, 1, mean)
# Calcula a média das colunas, coluna a coluna.
xa2 <- apply(x, 2, mean)
# Criando uma lista de matrizes.
# A função 't()' gera a matriz transposta.
lista_de_matrizes <- list(x = x, tx = t(x))
# Calculando a matriz inversa de ambas as matrizes.
lapply(lista_de_matrizes, solve)
# Calculando o determinante de ambas as matrizes.
lapply(lista_de_matrizes, det)
# Calculando o determinante de ambas as matrizes, agora com sapply().
sapply(lista_de_matrizes, det)
# Criando 1000 repetições de médias de 100 valores aleatórios.
set.seed(54321)
sims <- replicate(100000, mean(rnorm(100)))
# Visualizando o histograma.
hist(sims, col = "lightblue")
# Gerando objetos aleatórios.
x1 <- rnorm(100)
x2 <- rnorm(100)
# Aplicando a média na lista composta pelos dois objetos criados e
# eliminando frações dos resultados a cada um dos dois.
mapply(mean, x = list(x1, x2), trim = list(0.1, 0.2))
# A função acima equivale a:
mean(x1, trim = 0.1)
mean(x2, trim = 0.2)
dados <- read.csv2("Housing_Dataset.csv")
View(dados)
# Vamos inserir uma nova coluna contendo o preço por m2.
dados$pm2 <- dados$preco/dados$m2
# Analisando a estrutura do dataframe.
str(dados)
# Uma nova forma de olhar um subconjunto do dataframe.
subconjunto <- subset(dados, bairro == "Bairro_D", select=c(bairro, preco))
# Passando um filtro com índices lógicos.
# Selecionando os imóveis que estavam à venda
# com valor acima de de 1 milhão de reais.
selecao <- with(dados, tipo=="venda" & preco > 1000000)
sum(selecao) # quantos imóveis?
# Armazenando o filtro.
venda_maior1m <- dados[selecao,]
# Usando agora a função subset.
venda_maior1m_2 <- subset(dados, tipo == "venda" & preco > 1e6)
# Criando categorias discretas para a variável pm2 e salvando no dataframe.
# Ex.: até 50m2, de 50m2 a 100m2.
dados$m2_cat <- cut(dados$m2,
breaks= c(0, 50, 150, 200, 250, Inf),
labels=c("de 0 a 50 m2", "de 50 a 150 m2",
"de 150 a 200 m2","de 200 a 250 m2",
"mais de 250 m2") )
str(dados$m2_cat)
unicos <- na.omit(dados)
# 1) Separar a base em duas bases diferentes: aluguel e venda.
aluguel <- unicos[unicos$tipo == "aluguel", ]
venda <- unicos[unicos$tipo == "venda", ]
# 2) Calcular a média para cada um dos objetos.
media_aluguel <- mean(aluguel$preco)
media_venda <- mean(venda$preco)
# 3) Combinar os resultados em um único objeto.
medias = c(aluguel = media_aluguel, venda = media_venda)
medias
# Usando tapply.
# TAPPLY() - aplicar a função à subsetores de um vetor e estes são
# definidos por outro vetor.
tapply(X = unicos$preco, INDEX = unicos$tipo, FUN = mean)
# Usando aggregate.
aggregate(x = list(media = unicos$preco), by = list(tipo = unicos$tipo),
FUN = mean)
# Usando o pacote dplyr (mais sobre este pacote adiante).
library(dplyr)
unicos %>% group_by(tipo) %>% summarise(media = mean(preco))
# 1) Dividir o data.frame segundo uma lista de fatores
alug_venda <- split(unicos$preco, unicos$tipo)
View(alug_venda)
# Criamos uma lista alug_venda composta de dois vetores:
# um para aluguel e outro para venda.
str(alug_venda)
# Agora vamos aplicar uma função já conhecida a cada um dos
# elementos da lista.
medias_2 <- lapply(alug_venda, mean)
View(medias_2)
medias_2
# Queremos agora um vetor no lugar de uma lista.
# Podemos usar a função unlist().
unlist(medias_2)
# Se quisermos fazer as duas operações anteriores combinadas
# (olha a simplificação!), podemos usar sapply().
sapply(alug_venda, mean)
# Finalmente, vamos aplicar a função summary() para cada um dos elementos.
# Vamos analisar os resultados no PPTX.
lapply(alug_venda, summary)
# Eliminando outliers do dataframe.
# Filtrando a venda.
ok.venda <- with(unicos, tipo == "venda" &
pm2 > 3000 &
pm2 < 20000)
# Filtrando o aluguel.
ok.aluguel <- with(unicos, tipo == "aluguel" &
pm2 > 25 &
pm2 < 100)
# Juntando os dois resultados.
ok <- (ok.venda | ok.aluguel)
# Separando outliers e limpos.
outliers <- unicos[!ok,]
limpos <- unicos[ok,]
# Calculando a mediana do metro quadrado para aluguel e para venda
tapply(unicos$pm2, unicos$tipo, median)
# Agora queremos mediana para aluguel e venda, separada por bairro:
tapply(unicos$pm2, list(unicos$bairro, unicos$tipo), median)
# Vejamos aluguel e venda, separados por tipo de imóvel e por bairro.
# Criaremos um array de três dimensões:
tabelas <- tapply(unicos$pm2, list(unicos$imovel, unicos$tipo, unicos$bairro), median)
str(tabelas)
# Selecionando apenas o Bairro_K.
tabelas[ , ,"Bairro_K"]
# Selecionando apenas vendas por tipo de imóvel e bairro.
tabelas[ ,"venda", ]
View(unicos)
# Selecionando apenas o Bairro_K.
tabelas[ , ,"Bairro_K"]
# Selecionando apenas vendas por tipo de imóvel e bairro.
tabelas[ ,"venda", ]
# Agora perguntamos: qual a mediana do preço por metro quadrado
# dos apartamentos, separados por tamanho? E por quartos?
aps <- unicos[unicos$imovel == "apartamento", ]
aps2 <- limpos[limpos$imovel == "apartamento", ]
with(aps2, tapply(pm2, m2_cat, median))
with(aps2, tapply(pm2, quartos, median))
# Distribuição de apartamentos por quartos e metragem.
with(aps2, tapply(quartos, list(m2_cat, quartos), length))
# Calculando a mediana do preço por metro quadrado, separada por bairro,
# venda ou aluguel, e tipo de imóvel.
# Atenção para a diferença do formato deste resultado para o formato da tapply().
pm2_bairro_tipo_imovel <- aggregate(formula = pm2 ~ bairro + tipo + imovel,
data = unicos,
FUN = median)
setwd("C:/Users/barbosa/Google Drive/_Pos-graduacao/Bloco A/Analytics - Cassius Figueredo/MBA Big Data - Analytics com R (Aulas 05 e 06)/Arquivos Etapa 03b")
# Instalando os pacotes
library(arules)
install.packages("arules", dependencies=TRUE)
install.packages("arulesViz", dependencies=TRUE)
# Instalando os pacotes
library(arules)
library(arulesViz)
# Carregando o conjunto de dados.
groceries <- read.transactions(file = 'groceries.txt', sep = ',', rm.duplicates = T)
# Carregando o conjunto de dados.
groceries <- read.transactions(file = 'groceries.txt', sep = ',', rm.duplicates = T)
# Verificando os dados.
summary(groceries)
# Observando algumas transações.
# Cada linha da matriz representa uma cesta de compras.
inspect(groceries[1:4])
# Observando algumas transações.
# Cada linha da matriz representa uma cesta de compras.
inspect(groceries[1:10])
# Calculando o suporte para cada item e imprimindo gráfico de itens e seus suportes.
itemFrequency(groceries[,1:169])
itemFrequencyPlot(groceries[,1:169])
itemFrequencyPlot(groceries[,1:100])
itemFrequencyPlot(groceries[,1:10])
itemFrequencyPlot(groceries[,1:20])
#Visualizando gráfico de itens com suporte > 0,1.
itemFrequencyPlot(groceries, support = 0.1)
#Visualizando gráfico de itens com suporte > 0,1.
itemFrequencyPlot(groceries, support = 0.05)
#Visualizando gráfico de itens com suporte > 0,1.
itemFrequencyPlot(groceries, support = 0.06)
# Mais um gráfico, agora alterando os eixos.
itens <- itemFrequency(groceries)
itens
itens <- itens[order(itens, decreasing = T)] # Ordenando os itens.
dotchart(itens)
dotchart(itens[,1:10])
help(dotchart)
dotchart(itens, xlim = c(0, 1))
dotchart(itens, xlim = c(0, 0.1))
dotchart(itens, xlim = c(0, 1))
dotchart(itens, ylim = c(0, 1))
dotchart(itens, ylim = c(0, 0.02))
# Mesmos gráficos, menos itens.
# Pegando os 10 itens mais negociados.
dotchart(itens[1:10], cex = 0.8)
# Mesmos gráficos, menos itens.
# Pegando os 10 itens mais negociados.
dotchart(itens[1:20], cex = 0.8)
itemFrequencyPlot(groceries, topN=10)
itemFrequencyPlot(groceries, topN=20)
itemFrequencyPlot(groceries, topN=20, cex = 0.5)
itemFrequencyPlot(groceries, topN=20, cex = 0.6)
# Usando o algoritmos Apriori para descobrir as regras de associação.
regras <- apriori(groceries, parameter = list(support=0.001, confidence=0.5, minlen = 2))
# Verificando as regras.
summary(regras)
# Inspecionando as regras e ordenando-as por confiança e lift.
inspect(regras[1:10])
inspect(sort(regras[1:50], by = 'confidence'))
inspect(sort(regras[1:50], by = 'lift'))
# Visualizando as regras como um diagrama de dispersão.
plot(regras, method = 'scatterplot', shading=NA)
# Selecionando regras com confiança > 0,8.
subregras <- regras[quality(regras)$confidence > 0.8]
# Visualizando as regras em um gráfico suporte x confiança + ordem.
plot(subregras, method = 'two-key plot', shading=NA)
# Fazendo o gráfico de dispersão suporte por lift.
plot(regras, measure=c("support", "lift"), shading="confidence")
# Visualizando regras agrupadas.
plot(subregras, method="grouped", control = list(k=40), cex = 0.7)
plot(subregras, method="graph", control = list(k=40), cex = 0.7)
plot(subregras, method="graph", shading=NA)
plot(subregras, method="graph", control = list(k=40), shading=NA)
plot(subregras, method="graph", control = list(k=20), shading=NA)
# Visualizando regras agrupadas.
plot(subregras, method="grouped", control = list(k=40), cex = 0.7)
plot(subregras, method="graph", control = list(k=40), shading=NA)
plot(subregras[1:50], method="graph", shading=NA)
plot(subregras[1:20], method="graph", shading=NA)
setwd("C:/Users/barbosa/Google Drive/_Pos-graduacao/Bloco A/Analytics - Cassius Figueredo/MBA Big Data - Analytics com R (Aulas 05 e 06)/Arquivos Etapa 03c")
install.packages("party", dependencies=TRUE)
install.packages("rpart", dependencies=TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
install.packages("rpart", dependencies = TRUE)
library(rpart.plot) # visualização das árvores de decisão.
install.packages("rpart.plot", dependencies=TRUE)
install.packages("rattle", dependencies=TRUE)
